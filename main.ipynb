{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **2022 ML FALL HW1: PM2.5 Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqMEWsRekx0L"
      },
      "source": [
        "# **Fix random seed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "UxDA6fJb_Uem"
      },
      "outputs": [],
      "source": [
        "seed = 9487\n",
        "np.random.seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "yHpuZmQwXpz8"
      },
      "outputs": [],
      "source": [
        "def valid(x, y):\n",
        "  return True\n",
        "\n",
        "\n",
        "# Create your dataset\n",
        "def parse2train(data, feats):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n",
        "  total_length = data.shape[1] - 8\n",
        "\n",
        "  for i in range(total_length):\n",
        "    x_tmp = data[feats, i:i+8] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc.\n",
        "    y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n",
        "\n",
        "    # Filter out extreme values to train.\n",
        "    if valid(x_tmp, y_tmp):\n",
        "      x.append(x_tmp.reshape(-1,))\n",
        "      y.append(y_tmp)\n",
        "  \n",
        "  # x.shape: (n, 15, 8)\n",
        "  # y.shape: (n, 1) \n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return x,y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEpvVVQdZ0c"
      },
      "source": [
        "# **Adam**\n",
        "* Gradient descent algorithm. Adam was implemented.\n",
        "* Ref: https://arxiv.org/pdf/1412.6980.pdf\n",
        "\n",
        "![](https://i.imgur.com/jRaebdf.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "XL_RVBoLuXvj"
      },
      "outputs": [],
      "source": [
        "def minibatch(x, y, config):\n",
        "    # Randomize the data in minibatch\n",
        "    index_tr = np.arange(x.shape[0])[:80]\n",
        "    index_va = np.arange(x.shape[0])[-80:]\n",
        "    np.random.shuffle(index_tr)\n",
        "    x_tr = x[index_tr]\n",
        "    y_tr = y[index_tr]\n",
        "    x_va = x[index_va]\n",
        "    y_va = y[index_va]\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    lam = config.lam\n",
        "    epoch = config.epoch\n",
        "\n",
        "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n",
        "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    w1 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    w2 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    bias = 0.1\n",
        "    m_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    v_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
        "    m_t_b = 0.0\n",
        "    v_t_b = 0.0\n",
        "    t = 0\n",
        "    epsilon = 1e-8\n",
        "    # min\n",
        "    loss_va_min, w1b ,w2b, biasb = 3.6, 0, 0, 0\n",
        "    \n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        loss_va = 0\n",
        "        for b in range(int(x.shape[0]/batch_size)):\n",
        "            t+=1\n",
        "            x_batch = x_tr[b*batch_size:(b+1)*batch_size]\n",
        "            y_batch = y_tr[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
        "\n",
        "            # Prediction of linear regression \n",
        "            pred = np.dot(np.square(x_batch), w2) + np.dot(x_batch,w1) + bias\n",
        "            # loss\n",
        "            loss = y_batch - pred\n",
        "            \n",
        "            # Compute gradient\n",
        "            g_t_w1 = np.dot(x_batch.transpose(),loss) * (-2)\n",
        "            g_t_w2 = np.dot(x_batch.transpose(),loss) * (-2)\n",
        "            g_t_b = loss.sum(axis=0) * (-2)\n",
        "            m_t_w1 = beta_1*m_t + (1-beta_1)*g_t_w1\n",
        "            m_t_w2 = beta_1*m_t + (1-beta_1)*g_t_w2\n",
        "            v_t_w1 = beta_2*v_t + (1-beta_2)*np.multiply(g_t_w1, g_t_w1)\n",
        "            v_t_w2 = beta_2*v_t + (1-beta_2)*np.multiply(g_t_w2, g_t_w2)\n",
        "            m_cap_w1 = m_t_w1/(1-(beta_1**t))\n",
        "            m_cap_w2 = m_t_w2/(1-(beta_1**t))\n",
        "            v_cap_w1 = v_t_w1/(1-(beta_2**t))\n",
        "            v_cap_w2 = v_t_w2/(1-(beta_2**t))\n",
        "            m_t_b = 0.9*m_t_b + (1-0.9)*g_t_b\n",
        "            v_t_b = 0.99*v_t_b + (1-0.99)*(g_t_b*g_t_b) \n",
        "            m_cap_b = m_t_b/(1-(0.9**t))\n",
        "            v_cap_b = v_t_b/(1-(0.99**t))\n",
        "            w1_0 = np.copy(w1)\n",
        "            w2_0 = np.copy(w2)\n",
        "            # Update weight & bias\n",
        "            w1 -= ((lr*m_cap_w1)/(np.sqrt(v_cap_w1)+epsilon)).reshape(-1, 1)\n",
        "            w2 -= ((lr*m_cap_w2)/(np.sqrt(v_cap_w2)+epsilon)).reshape(-1, 1)\n",
        "            bias -= (lr*m_cap_b)/(math.sqrt(v_cap_b)+epsilon)\n",
        "        \n",
        "        pred_va = np.dot(np.square(x_va), w2) + np.dot(x_va,w1) + bias\n",
        "        \n",
        "        loss_va = np.mean(y_va.reshape(-1,1) - pred_va)\n",
        "        print(f\"epoch{num}, valid loss:{loss_va}\") if num%300 == 0 else None\n",
        "        if abs(loss_va_min) > abs(loss_va) and loss_va > 0 and num > 1000:\n",
        "          w1b = w1\n",
        "          w2b = w2\n",
        "          biasb = bias\n",
        "          loss_va_min = loss_va\n",
        "          print(f\"save best at {num}, loss = {loss_va_min}\")\n",
        "\n",
        "    return w1, w2, bias, w1b, w2b, biasb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "ZpdOsMfXLxH2"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# TODO: Tune the config to boost your performance. \n",
        "train_config = Namespace(\n",
        "    batch_size = 256,\n",
        "    lr = 1e-4,\n",
        "    lam = 0.0001,\n",
        "    epoch = 15000,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay-RhqqA88vS"
      },
      "source": [
        "# **Training regression model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoR5Q5kvJm4t",
        "outputId": "e70040ac-6191-456e-f8ce-e450bc87b468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AMB_TEMP     -0.176147\n",
              "CO            0.659148\n",
              "NO            0.227219\n",
              "NO2           0.554274\n",
              "NOx           0.513650\n",
              "O3            0.233924\n",
              "PM10          0.818868\n",
              "WS_HR        -0.102047\n",
              "RAINFALL     -0.060801\n",
              "RH           -0.081576\n",
              "SO2           0.361333\n",
              "WD_HR         0.171932\n",
              "WIND_DIREC    0.137658\n",
              "WIND_SPEED   -0.101197\n",
              "PM2.5         1.000000\n",
              "Name: PM2.5, dtype: float64"
            ]
          },
          "execution_count": 350,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(f\"{os.getcwd()}/train.csv\")\n",
        "data.corr().loc[\"PM2.5\",:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Akqj5yYVGHA"
      },
      "outputs": [],
      "source": [
        "feats = [1, 3, 6, 14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "AiEWGMQXLM99"
      },
      "outputs": [],
      "source": [
        "# Training data preprocessing.\n",
        "\n",
        "data = data.values\n",
        "train_data = np.transpose(np.array(np.float64(data)))\n",
        "train_x, train_y = parse2train(train_data, feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhfoPJUhcnH9",
        "outputId": "4fda3e88-b88c-4721-eb2e-518bfb6535fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch0, valid loss:-641.6025262422852\n",
            "epoch300, valid loss:-446.6767541651931\n",
            "epoch600, valid loss:-250.25116182245856\n",
            "epoch900, valid loss:-53.82726135351679\n",
            "save best at 1002, loss = 0.5600952567850807\n",
            "save best at 1004, loss = 0.5241357436440943\n",
            "save best at 1006, loss = 0.4980161736383926\n",
            "save best at 1008, loss = 0.46067455028549187\n",
            "save best at 1010, loss = 0.43450357003262924\n",
            "save best at 1012, loss = 0.39712530712480576\n",
            "save best at 1014, loss = 0.3710006154477143\n",
            "save best at 1016, loss = 0.37025880369055064\n",
            "save best at 1018, loss = 0.3695119300680127\n",
            "save best at 1020, loss = 0.3687608882618502\n",
            "save best at 1022, loss = 0.3680074037217198\n",
            "save best at 1024, loss = 0.3672526320069175\n",
            "save best at 1026, loss = 0.36649733210746394\n",
            "save best at 1028, loss = 0.36574199942515534\n",
            "save best at 1030, loss = 0.36498695626214306\n",
            "save best at 1032, loss = 0.36423241209563606\n",
            "save best at 1034, loss = 0.36347850321908926\n",
            "save best at 1036, loss = 0.3627253186033167\n",
            "save best at 1038, loss = 0.3619729166809169\n",
            "save best at 1040, loss = 0.36122133620399194\n",
            "save best at 1042, loss = 0.36047060325369124\n",
            "save best at 1044, loss = 0.35972073576154956\n",
            "save best at 1046, loss = 0.35897174643453494\n",
            "save best at 1048, loss = 0.35822364491940345\n",
            "save best at 1050, loss = 0.3575464118660817\n",
            "save best at 1052, loss = 0.35686990707169725\n",
            "save best at 1082, loss = 0.3565670793979602\n",
            "save best at 1084, loss = 0.35580716271299184\n",
            "save best at 1086, loss = 0.35504812149243453\n",
            "save best at 1088, loss = 0.35428995297213217\n",
            "save best at 1090, loss = 0.35353265651216903\n",
            "save best at 1092, loss = 0.35277623283968446\n",
            "save best at 1094, loss = 0.35202068356206045\n",
            "save best at 1096, loss = 0.3512660108539035\n",
            "save best at 1098, loss = 0.3505122172557745\n",
            "save best at 1100, loss = 0.3497593055447984\n",
            "save best at 1102, loss = 0.3490072786515742\n",
            "save best at 1104, loss = 0.3482561396071349\n",
            "save best at 1106, loss = 0.34750589151017774\n",
            "save best at 1108, loss = 0.3467565375129641\n",
            "save best at 1110, loss = 0.346008080922636\n",
            "save best at 1112, loss = 0.34533049862632154\n",
            "save best at 1114, loss = 0.34465364564674517\n",
            "save best at 1116, loss = 0.3439775429202628\n",
            "save best at 1118, loss = 0.3433022122443173\n",
            "save best at 1120, loss = 0.3426276685786934\n",
            "save best at 1122, loss = 0.34195392241093525\n",
            "save best at 1124, loss = 0.34128098135132534\n",
            "save best at 1126, loss = 0.3406088511598298\n",
            "save best at 1128, loss = 0.33993753640657637\n",
            "save best at 1130, loss = 0.33926704089653403\n",
            "save best at 1132, loss = 0.3385973679425579\n",
            "save best at 1134, loss = 0.33792852054091677\n",
            "save best at 1136, loss = 0.3372605014841692\n",
            "save best at 1138, loss = 0.33659331343377374\n",
            "save best at 1140, loss = 0.3359269589668908\n",
            "save best at 1142, loss = 0.3352614406067419\n",
            "save best at 1144, loss = 0.3345967608426914\n",
            "save best at 1146, loss = 0.33393292214453607\n",
            "save best at 1148, loss = 0.33326992697557856\n",
            "save best at 1150, loss = 0.33260777781536266\n",
            "save best at 1152, loss = 0.3319464772641352\n",
            "save best at 1154, loss = 0.33128603749005653\n",
            "save best at 1186, loss = 0.33073397823752976\n",
            "save best at 1188, loss = 0.330059162925014\n",
            "save best at 1190, loss = 0.3293851550017635\n",
            "save best at 1192, loss = 0.32871195828538713\n",
            "save best at 1194, loss = 0.32803957609754864\n",
            "save best at 1196, loss = 0.32736801144079586\n",
            "save best at 1198, loss = 0.32669726711225006\n",
            "epoch1200, valid loss:0.32602734577667436\n",
            "save best at 1200, loss = 0.32602734577667436\n",
            "save best at 1202, loss = 0.3253582500134156\n",
            "save best at 1204, loss = 0.3246899823465565\n",
            "save best at 1206, loss = 0.32402254526426977\n",
            "save best at 1208, loss = 0.32335594123123695\n",
            "save best at 1210, loss = 0.32269017269660505\n",
            "save best at 1212, loss = 0.3220252420990865\n",
            "save best at 1214, loss = 0.3213611518702359\n",
            "save best at 1216, loss = 0.3206979044365492\n",
            "save best at 1218, loss = 0.3200355022208411\n",
            "save best at 1220, loss = 0.3193739476431695\n",
            "save best at 1222, loss = 0.31871324312151866\n",
            "save best at 1224, loss = 0.318053391072394\n",
            "save best at 1226, loss = 0.31739439391146446\n",
            "save best at 1228, loss = 0.3167362540544656\n",
            "save best at 1230, loss = 0.31607897391864126\n",
            "save best at 1232, loss = 0.3154225559254068\n",
            "save best at 1234, loss = 0.3147670025057098\n",
            "save best at 1236, loss = 0.31411231611229606\n",
            "save best at 1238, loss = 0.3134584992530448\n",
            "save best at 1240, loss = 0.3128055546109253\n",
            "save best at 1242, loss = 0.31215348581051006\n",
            "epoch1500, valid loss:0.3854719224009603\n",
            "save best at 1762, loss = 0.31160117101903956\n",
            "save best at 1764, loss = 0.31100057977409296\n",
            "save best at 1766, loss = 0.3104009877211846\n",
            "save best at 1768, loss = 0.3098023971256649\n",
            "save best at 1770, loss = 0.3092048108772275\n",
            "save best at 1772, loss = 0.30860824727315384\n",
            "epoch1800, valid loss:0.3786761063488041\n",
            "save best at 2064, loss = 0.30833294885946544\n",
            "save best at 2066, loss = 0.30775637741901557\n",
            "save best at 2068, loss = 0.30718084938867707\n",
            "save best at 2070, loss = 0.30660636651800943\n",
            "save best at 2072, loss = 0.30603293054657155\n",
            "save best at 2074, loss = 0.3054605432055427\n",
            "save best at 2076, loss = 0.30488920622074317\n",
            "save best at 2078, loss = 0.3043189213187105\n",
            "save best at 2080, loss = 0.30374969024024734\n",
            "save best at 2082, loss = 0.30318151477575767\n",
            "save best at 2084, loss = 0.3026143968840416\n",
            "save best at 2086, loss = 0.3020483393459411\n",
            "save best at 2088, loss = 0.30148338452630213\n",
            "epoch2100, valid loss:0.38814114102989905\n",
            "save best at 2380, loss = 0.30133871640216425\n",
            "save best at 2382, loss = 0.3007960070562829\n",
            "save best at 2384, loss = 0.3002543894215334\n",
            "save best at 2386, loss = 0.29971386478084455\n",
            "save best at 2388, loss = 0.2991744344043889\n",
            "save best at 2390, loss = 0.2986360995516498\n",
            "save best at 2392, loss = 0.2980988614755179\n",
            "save best at 2394, loss = 0.2975627214314265\n",
            "save best at 2396, loss = 0.29702768070244606\n",
            "save best at 2398, loss = 0.2964937407040306\n",
            "epoch2400, valid loss:0.2959609049205384\n",
            "save best at 2400, loss = 0.2959609049205384\n",
            "save best at 2440, loss = 0.2958100236260569\n",
            "save best at 2442, loss = 0.29525270704174095\n",
            "save best at 2444, loss = 0.2946964649249261\n",
            "save best at 2446, loss = 0.29414129821184326\n",
            "save best at 2448, loss = 0.2935872080202973\n",
            "save best at 2450, loss = 0.29303419557925026\n",
            "save best at 2452, loss = 0.29248226218356554\n",
            "save best at 2454, loss = 0.2919314091649223\n",
            "save best at 2456, loss = 0.2913816378731073\n",
            "save best at 2458, loss = 0.2908329496639962\n",
            "save best at 2460, loss = 0.2902853458918092\n",
            "save best at 2462, loss = 0.2897388279041411\n",
            "save best at 2464, loss = 0.28919339703877767\n",
            "save best at 2466, loss = 0.28864905462166024\n",
            "save best at 2468, loss = 0.28810580196563734\n",
            "save best at 2470, loss = 0.2875636403697331\n",
            "save best at 2472, loss = 0.2870225711188129\n",
            "save best at 2474, loss = 0.2864825954835909\n",
            "save best at 2476, loss = 0.285943714721022\n",
            "save best at 2478, loss = 0.28540593007521176\n",
            "save best at 2480, loss = 0.2848692427792395\n",
            "save best at 2482, loss = 0.28433365405872724\n",
            "save best at 2484, loss = 0.28379916513936504\n",
            "save best at 2486, loss = 0.28326577726461416\n",
            "save best at 2488, loss = 0.2827334917458063\n",
            "save best at 2490, loss = 0.28220231015550823\n",
            "save best at 2492, loss = 0.2816722358071154\n",
            "epoch2700, valid loss:0.3357267223020085\n",
            "epoch3000, valid loss:0.38168760113469385\n",
            "epoch3300, valid loss:0.3156547968421485\n",
            "epoch3600, valid loss:0.3674739247917767\n",
            "epoch3900, valid loss:0.311162892436788\n",
            "epoch4200, valid loss:0.35735397436408667\n",
            "epoch4500, valid loss:0.32174119548603064\n",
            "epoch4800, valid loss:0.37435622367625354\n",
            "epoch5100, valid loss:0.3341686039523875\n",
            "epoch5400, valid loss:0.39260739787101206\n",
            "epoch5700, valid loss:0.3579961019804908\n",
            "epoch6000, valid loss:0.42704537596224784\n",
            "epoch6300, valid loss:0.38185834792839224\n",
            "epoch6600, valid loss:0.35922250463018945\n",
            "epoch6900, valid loss:0.4294001929849293\n",
            "epoch7200, valid loss:0.4054069493552347\n",
            "epoch7500, valid loss:0.3885106784292853\n",
            "epoch7800, valid loss:0.47748315273238173\n",
            "epoch8100, valid loss:0.4364426260912181\n",
            "epoch8400, valid loss:0.4224622763615587\n",
            "epoch8700, valid loss:0.3986161417838855\n",
            "epoch9000, valid loss:0.48190254348942574\n",
            "epoch9300, valid loss:0.45024706675134896\n",
            "epoch9600, valid loss:0.43171428059952\n",
            "epoch9900, valid loss:0.42698507799490715\n",
            "epoch10200, valid loss:0.412818851478675\n",
            "epoch10500, valid loss:0.5133507750343933\n",
            "epoch10800, valid loss:0.48377523654442134\n",
            "epoch11100, valid loss:0.4665214998764252\n",
            "epoch11400, valid loss:0.4629769423696978\n",
            "epoch11700, valid loss:0.4492812465424919\n",
            "epoch12000, valid loss:0.44144126989146404\n",
            "epoch12300, valid loss:0.43696878790871974\n",
            "epoch12600, valid loss:0.4431243129380233\n",
            "epoch12900, valid loss:0.43260175856296107\n",
            "epoch13200, valid loss:0.42659084465023805\n",
            "epoch13500, valid loss:0.4231641802287108\n",
            "epoch13800, valid loss:0.42121207794628024\n",
            "epoch14100, valid loss:0.4201002963332172\n",
            "epoch14400, valid loss:0.4194671956647932\n",
            "epoch14700, valid loss:0.4191067299984006\n"
          ]
        }
      ],
      "source": [
        "# Train regression model\n",
        "\n",
        "w1, w2, bias, w1b, w2b, biasb = minibatch(train_x, train_y, train_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019GwPMrbmrB"
      },
      "source": [
        "# **Testing:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "id": "5FjQNzOb6BeQ"
      },
      "outputs": [],
      "source": [
        "def parse2test(data, feats):\n",
        "  x = []\n",
        "  for i in range(90):\n",
        "    x_tmp = data[feats,8*i: 8*i+8]\n",
        "    x.append(x_tmp.reshape(-1,))\n",
        "  # x.shape: (n, 15, 8)\n",
        "  x = np.array(x)\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "id": "z40o9QbAYbR6"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('test.csv')\n",
        "data = data.values\n",
        "\n",
        "test_data = np.transpose(np.array(np.float64(data)))\n",
        "test_x = parse2test(test_data, feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWrfEwaEdO6J"
      },
      "source": [
        "# **Write result as .csv**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEQ1fZ9-WMO",
        "outputId": "94eaa4bf-a251-4e59-dc8b-6fa372ae3a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(90, 32)\n"
          ]
        }
      ],
      "source": [
        " with open('re.csv', 'w', newline='') as csvf:\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow(['Id','Predicted'])\n",
        "    print(test_x.shape) \n",
        "    for i in range(int(test_x.shape[0])): \n",
        "      prediction = ( np.dot(np.reshape(w2b,-1),np.square(test_x[i])) + np.dot(np.reshape(w1b,-1),test_x[i]) + biasb)[0]\n",
        "      writer.writerow([i, prediction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UqKEeP5lr9f",
        "outputId": "53de8666-53c7-44fc-d959-eb7f5f417af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w2\n",
            " [[ 0.00342926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00222925]\n",
            " [ 0.00162926]\n",
            " [ 0.00282926]\n",
            " [ 0.00482926]\n",
            " [ 0.03242926]\n",
            " [ 0.00402926]\n",
            " [ 0.00282926]\n",
            " [ 0.00082926]\n",
            " [-0.00157076]\n",
            " [-0.00337075]\n",
            " [ 0.00082926]\n",
            " [ 0.00262926]\n",
            " [ 0.00282926]\n",
            " [ 0.00062926]\n",
            " [ 0.00082926]\n",
            " [ 0.00042926]\n",
            " [-0.00117074]\n",
            " [ 0.00162926]\n",
            " [ 0.00262926]\n",
            " [ 0.00282926]\n",
            " [ 0.00202926]\n",
            " [ 0.00302926]\n",
            " [ 0.00302926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00582926]] \n",
            "w1\n",
            " [[ 0.00342926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00222925]\n",
            " [ 0.00162926]\n",
            " [ 0.00282926]\n",
            " [ 0.00482926]\n",
            " [ 0.03242926]\n",
            " [ 0.00402926]\n",
            " [ 0.00282926]\n",
            " [ 0.00082926]\n",
            " [-0.00157076]\n",
            " [-0.00337075]\n",
            " [ 0.00082926]\n",
            " [ 0.00262926]\n",
            " [ 0.00282926]\n",
            " [ 0.00062926]\n",
            " [ 0.00082926]\n",
            " [ 0.00042926]\n",
            " [-0.00117074]\n",
            " [ 0.00162926]\n",
            " [ 0.00262926]\n",
            " [ 0.00282926]\n",
            " [ 0.00202926]\n",
            " [ 0.00302926]\n",
            " [ 0.00302926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00282926]\n",
            " [ 0.00582926]] \n",
            "bias\n",
            " [1.44646702]\n"
          ]
        }
      ],
      "source": [
        "print(\"w2\\n\", w2b, \"\\nw1\\n\", w1b, \"\\nbias\\n\", biasb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('3.9.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1b6f505cb9e4a0105833e71df224b61349e0c214af5a02524b56a975591352f4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
